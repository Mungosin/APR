{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import types\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import Surface\n",
    "import plotly.tools as tls\n",
    "import sympy\n",
    "from sympy import *\n",
    "tls.set_credentials_file(username='Mungos', api_key='mcnflyescl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nelder Mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nelder_mead(f, x_start,\n",
    "                step=1, no_improve_thr=10e-6,\n",
    "                no_improv_break=50, max_iter=0,\n",
    "                alpha=1., gamma=2., rho=-0.5, sigma=0.5,verbose=False):\n",
    "    \n",
    "    dim = len(x_start)\n",
    "    prev_best = f.evaluate(x_start)\n",
    "    no_improv = 0\n",
    "    x_start = np.array(x_start).astype(np.float64)\n",
    "    res = [[x_start, prev_best]]\n",
    "\n",
    "    for i in range(dim):\n",
    "        x = deepcopy(x_start)\n",
    "        x[i] = x[i] + step\n",
    "        score = f.evaluate(x)\n",
    "        res.append([x, score])\n",
    "    \n",
    "    \n",
    "    # simplex iter\n",
    "    iters = 0\n",
    "    while 1:\n",
    "        # order\n",
    "        res.sort(key=lambda x: x[1])\n",
    "        best = res[0][1]\n",
    "\n",
    "        # break after max_iter\n",
    "        if max_iter and iters >= max_iter:\n",
    "            return res[0][0]\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "        if best < prev_best - no_improve_thr:\n",
    "            no_improv = 0\n",
    "            prev_best = best\n",
    "        else:\n",
    "            no_improv += 1\n",
    "\n",
    "        if no_improv >= no_improv_break:\n",
    "            return res[0][0]\n",
    "\n",
    "        # centroid\n",
    "        x0 = [0.] * dim\n",
    "        for tup in res[:-1]:\n",
    "            for i, c in enumerate(tup[0]):\n",
    "                x0[i] += c / (len(res)-1)\n",
    "                \n",
    "        # break after no_improv_break iterations with no improvement\n",
    "        if verbose:\n",
    "            print  'Value in centroid: ', f.evaluate(x0), ' Centroid: ', x0\n",
    "                \n",
    "        # reflection\n",
    "        xr = x0 + alpha*(x0 - res[-1][0])\n",
    "        rscore = f.evaluate(xr)\n",
    "        if rscore < res[0][1]:\n",
    "            xe = x0 + gamma*(x0 - res[-1][0])\n",
    "            escore = f.evaluate(xe)\n",
    "            if escore < res[0][1]:\n",
    "                del res[-1]\n",
    "                res.append([xe, escore])\n",
    "                continue\n",
    "            else:\n",
    "                del res[-1]\n",
    "                res.append([xr, rscore])\n",
    "                continue\n",
    "        else:\n",
    "            if rscore > res[-2][1]:\n",
    "                if rscore < res[-1][1]:\n",
    "                    del res[-1]\n",
    "                    res.append([xr, rscore])\n",
    "                # contraction\n",
    "                xc = x0 + rho*(x0 - res[-1][0])\n",
    "                cscore = f.evaluate(xc)\n",
    "                if cscore < res[-1][1]:\n",
    "                    del res[-1]\n",
    "                    res.append([xc, cscore])\n",
    "                    continue\n",
    "                else:\n",
    "                    # reduction\n",
    "                    x1 = res[0][0]\n",
    "                    nres = []\n",
    "                    for tup in res:\n",
    "                        redx = x1 + sigma*(tup[0] - x1)\n",
    "                        score = f.evaluate(redx)\n",
    "                        nres.append([redx, score])\n",
    "                    res = nres\n",
    "            else:\n",
    "                del res[-1]\n",
    "                res.append([xr, rscore])\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "# #     fun = Function(f4)\n",
    "#     print nelder_mead(Rosenbrock(), [5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def golden_section_search(f,starting_point = None, a=None,b=None,eps=1, verbose = False):\n",
    "    if starting_point != None:\n",
    "        a,b = find_unimodal(starting_point,f.evaluate,eps)\n",
    "    elif a==None or b== None:\n",
    "        raise Exception(\"Starting point or unimodal interval needs to be given\")\n",
    "    \n",
    "    if verbose:\n",
    "        print \"Searching in interval [%f, %f]\" % (a,b)\n",
    "    \n",
    "    \n",
    "    fi = (math.sqrt(5.0) - 1.0)/2\n",
    "    c = b - (b - a)*fi\n",
    "    d = a + (b - a)*fi\n",
    "    while (b - a) > eps:\n",
    "        c_score, d_score = f.evaluate(c),f.evaluate(d)\n",
    "        if verbose:\n",
    "            print \"|a = %.3f|c = %.3f|d = %.3f|b = %.3f|f(c) = %.3f|f(d) = %.3f|f(c) > f(d) = %s\" %(a,c,d,b,c_score,d_score, c_score>d_score)\n",
    "        if c_score >= d_score:\n",
    "            a = c\n",
    "            c = d\n",
    "            d = a + (b - a)*fi\n",
    "        else:\n",
    "            b = d\n",
    "            d = c\n",
    "            c = b - (b - a)*fi\n",
    "    if verbose:\n",
    "        print ''\n",
    "        print \"Final interval = [\",a,\", \", b,\"]\"\n",
    "    return (a+b)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unimodalni interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_unimodal(starting_point, function, starting_step=1):\n",
    "    step = starting_step\n",
    "    current_point = starting_point\n",
    "    direction = 1\n",
    "    if function(current_point + step) >= function(current_point):\n",
    "        direction=-1\n",
    "    next_point = current_point + direction*step\n",
    "    cnt = 1\n",
    "    previous_point = current_point\n",
    "    while function(current_point) > function(next_point):\n",
    "        previous_point = current_point\n",
    "        current_point = next_point\n",
    "        next_point = starting_point + direction * (2**cnt) * step\n",
    "        cnt+=1\n",
    "    \n",
    "    if previous_point > next_point:\n",
    "        return next_point, previous_point\n",
    "    return previous_point, next_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base function class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Function():\n",
    "    def subs_dict(self,x):\n",
    "        dic = {}\n",
    "        for i,var in enumerate(self.variables):\n",
    "            dic[var] = x[i]\n",
    "        return dic\n",
    "    \n",
    "    def gradient_symbolic(self):\n",
    "        grads = []\n",
    "        for var in self.variables:\n",
    "            dif = diff(self.equation,var)\n",
    "            grads.append(dif)\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.gradient_symbolic()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "        \n",
    "    def hessean_symbolic(self):\n",
    "        grads = self.gradient_symbolic()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in self.variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "        \n",
    "    def hessean(self,x):\n",
    "        self.hessean_calls+=1\n",
    "        hess = self.hessean_symbolic()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def evaluate(self,x, exponential=False):\n",
    "        self.calls+=1\n",
    "        subs = self.subs_dict(x)\n",
    "        value = np.array(self.equation.evalf(subs=subs),dtype=np.float32)\n",
    "        return value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_numeric(x):\n",
    "    NumberTypes = (types.IntType, types.LongType, types.FloatType)\n",
    "    return isinstance(x, NumberTypes)\n",
    "\n",
    "class BoxOptFunction():\n",
    "    def __init__(self, func,inequality_restriction):\n",
    "        self.calls = 0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.inequality_restriction = inequality_restriction\n",
    "        self.func = func\n",
    "        \n",
    "    def check_inequality_constraints(self,x):\n",
    "        for constraint in self.inequality_restriction:\n",
    "            if(constraint.evaluate(x)<0):\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "\n",
    "    def evaluate(self,X):\n",
    "        self.calls+=1\n",
    "        return self.func.evaluate(X)\n",
    "    \n",
    "\n",
    "class MixedTransformationFunction():\n",
    "    def __init__(self, func,inequality_restriction=None, equality_restriction=None):\n",
    "        self.calls = 0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.t = 1\n",
    "        self.inequality_restriction = inequality_restriction\n",
    "        self.equality_restriction = equality_restriction\n",
    "        self.func = func\n",
    "        \n",
    "    def combined_symbolic_grad(self): \n",
    "        func_grads = self.func.gradient_symbolic()\n",
    "        if self.inequality_restriction:\n",
    "            for eq in self.inequality_restriction:\n",
    "                for i in range(len(func_grads)):\n",
    "                    func_grads[i] -= (1./self.t*eq.gradient_symbolic())[i]\n",
    "                \n",
    "        if self.equality_restriction:\n",
    "            for eq in self.equality_restriction:\n",
    "                for i in range(len(func_grads)):\n",
    "                    func_grads[i] +=self.t*eq.gradient_symbolic()[i]\n",
    "        return func_grads\n",
    "    \n",
    "    def combined_symbolic_hessean(self):\n",
    "        grads = self.combined_symbolic_grad()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in self.func.variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "    \n",
    "    def combined_grad(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.combined_symbolic_grad()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "    \n",
    "    def hessean(self,x):\n",
    "        self.hessean_calls+=1\n",
    "        hess = self.combined_symbolic_hessean()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def set_t(self,t):\n",
    "        self.t = t\n",
    "        \n",
    "    def evaluate(self,X):\n",
    "        self.calls+=1\n",
    "        func_value = self.func.evaluate(X)\n",
    "        if self.inequality_restriction:\n",
    "            constraint_sum = 0\n",
    "            for eq in self.inequality_restriction:\n",
    "                try:\n",
    "                    val = eq.evaluate(X)\n",
    "                except TypeError:\n",
    "                    return 1e10\n",
    "                constraint_sum +=val\n",
    "            func_value -= 1./self.t*constraint_sum\n",
    "                \n",
    "        if self.equality_restriction:\n",
    "            constraint_sum=0\n",
    "            for eq in self.equality_restriction:\n",
    "                val = eq.evaluate(X)\n",
    "                constraint_sum +=val\n",
    "            func_value += self.t*constraint_sum\n",
    "        return func_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rosenbrock(Function):\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        self.equation = (100*(self.x2-self.x1**2)**2+(1-self.x1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F2(Function):\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        self.equation = ((self.x1-4.)**2 + 4.*(self.x2-2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F3(Function):\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        self.equation = ((self.x1-2.)**2 + (self.x2+3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F4(Function):\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        self.equation = ((self.x1-3.)**2 + (self.x2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restrictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class First_restriction(Function): #x2-x1 >= 0\n",
    "    def __init__(self, inequality_rest=True,power=2):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        if inequality_rest:\n",
    "            self.equation = log(self.x2-self.x1)\n",
    "        else:\n",
    "            self.equation = (self.x2-self.x1)**power\n",
    "        \n",
    "class Second_Restriction(Function): #x2-x1 >= 0\n",
    "    def __init__(self, inequality_rest=True,power=2):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        if inequality_rest:\n",
    "            self.equation = log((2-self.x1))\n",
    "        else:\n",
    "            self.equation = (2-self.x1)**power\n",
    "       \n",
    "    \n",
    "class Third_Restriction(Function): #3-x1-x2 >= 0\n",
    "    def __init__(self, inequality_rest=True,power=2):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        if inequality_rest:\n",
    "            self.equation = log((3-self.x1-self.x2))\n",
    "        else:\n",
    "            self.equation = (3-self.x1-self.x2)**power\n",
    "            \n",
    "            \n",
    "class Fourth_Restriction(Function): #3+1.5*x1-x2 >= 0\n",
    "    def __init__(self, inequality_rest=True,power=2):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        if inequality_rest:\n",
    "            self.equation = log((3+1.5*self.x1-self.x2))\n",
    "        else:\n",
    "            self.equation = (3+1.5*self.x1-self.x2)**power\n",
    "            \n",
    "class Fifth_Restriction(Function): #3-x1-x2 >= 0\n",
    "    def __init__(self, inequality_rest=True,power=2):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.x1= symbols('x1')\n",
    "        self.x2 = symbols('x2')\n",
    "        self.variables = [self.x1,self.x2]\n",
    "        if inequality_rest:\n",
    "            self.equation = log((self.x2-1))\n",
    "        else:\n",
    "            self.equation = (self.x2-1)**power\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trazenje unutarnje tocke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Inner_Point_Search():\n",
    "    def __init__(self, nejednadzbe,t=None):\n",
    "        self.nejed = nejednadzbe\n",
    "        if t:\n",
    "            self.t = t\n",
    "        else:\n",
    "            self.t = 1\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        total_val = 0\n",
    "        for nejed in self.nejed:\n",
    "            value = nejed.evaluate(x)\n",
    "            if value < 0:\n",
    "                total_val -= value * self.t\n",
    "        return total_val\n",
    "    \n",
    "    def get_inner_point(self,dimensions,iters=30):\n",
    "        point = np.random.randint(-10,10,size=dimensions)\n",
    "        for i in range(iters):\n",
    "            point = nelder_mead(self, point)\n",
    "            if evaluate(point) == 0:\n",
    "                return point\n",
    "            self.t *=2\n",
    "        return point\n",
    "    \n",
    "# nejed = [F2()]\n",
    "# inner = Inner_Point_Search(nejed,5)\n",
    "# print inner.get_inner_point(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradijentni spust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_descent(fun, starting_point, epsilon = 1e-6, use_golden_section=True, patience = 50, verbose = False, grad_norm = 1e-2):\n",
    "    norma = np.linalg.norm(fun.gradient(starting_point))\n",
    "    current_point = np.array(starting_point).copy()\n",
    "    if norma < epsilon:\n",
    "        print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "        return current_point\n",
    "    \n",
    "    best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    current_patience = 0\n",
    "    while True:\n",
    "        gradient = fun.gradient(current_point)\n",
    "        norma = np.linalg.norm(gradient)\n",
    "        if verbose:\n",
    "            print \"Current best point %s with value %s\" % (best_sol,best_sol_value)\n",
    "        if norma < grad_norm:\n",
    "            print \"Euclidean norm %s less than od %s stopping search\" % (norma, grad_norm)\n",
    "            return best_sol\n",
    "        \n",
    "        \n",
    "        if use_golden_section:\n",
    "            gradient_normed = gradient/np.linalg.norm(gradient)\n",
    "            \n",
    "            def function_1D_wrapper(function, array,grads):\n",
    "                cpy = array.copy()\n",
    "                class Decorator:\n",
    "                    def __init__(self,cpy,grads,function):\n",
    "                        self.grads = grads\n",
    "                        self.cpy = cpy\n",
    "                        self.function = function\n",
    "\n",
    "                    def evaluate(self,x):\n",
    "                        val = self.cpy - x * self.grads\n",
    "                        return self.function.evaluate(val)\n",
    "                return Decorator(cpy,grads,function)\n",
    "            \n",
    "            optimal_lambda = golden_section_search(function_1D_wrapper(fun,current_point,gradient_normed),1,eps=1e-6)\n",
    "            current_point -= optimal_lambda*gradient_normed\n",
    "        else:\n",
    "            current_point-= gradient\n",
    "            \n",
    "        if np.isnan(current_point).any() or (current_point==np.inf).any():\n",
    "            print \"Divergence detected, stopping search with best found solution %s\" %(best_sol)\n",
    "            return best_sol\n",
    "        \n",
    "        if best_sol_value <= fun.evaluate(current_point):\n",
    "            current_patience +=1\n",
    "            if(current_patience > patience):\n",
    "                print \"No improvement, stopping search with best found solution %s\" %(best_sol)\n",
    "                return best_sol\n",
    "        else:\n",
    "            current_patience = 0\n",
    "            best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    \n",
    "            \n",
    "#grad_descent(F2(),[3.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Rhapson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def newton_rhapson(fun, starting_point, epsilon = 1e-6, use_golden_section=True, patience = 50, verbose = False, grad_norm = 1e-2):\n",
    "    gradient = fun.gradient(starting_point).dot(np.linalg.inv(fun.hessean(starting_point)))\n",
    "    norma = np.linalg.norm(gradient)\n",
    "    current_point = np.array(starting_point, dtype=np.float32).copy()\n",
    "    if norma < epsilon:\n",
    "        print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "        return current_point\n",
    "    \n",
    "    best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    current_patience = 0\n",
    "    while True:\n",
    "        gradient = fun.gradient(current_point).dot(np.linalg.inv(fun.hessean(current_point)))\n",
    "        norma = np.linalg.norm(gradient)\n",
    "        if verbose:\n",
    "            print \"Current best point %s with value %s\" % (best_sol,best_sol_value)\n",
    "        if norma < grad_norm:\n",
    "            print \"Euclidean norm %s less than od %s stopping search\" % (norma, grad_norm)\n",
    "            return current_point\n",
    "        \n",
    "        \n",
    "        if use_golden_section:\n",
    "            gradient_normed = gradient/np.linalg.norm(gradient)\n",
    "            \n",
    "            def function_1D_wrapper(function, array,grads):\n",
    "                cpy = array.copy()\n",
    "                class Decorator:\n",
    "                    def __init__(self,cpy,grads,function):\n",
    "                        self.grads = grads\n",
    "                        self.cpy = cpy\n",
    "                        self.function = function\n",
    "\n",
    "                    def evaluate(self,x):\n",
    "                        val = self.cpy - x * self.grads\n",
    "                        return self.function.evaluate(val)\n",
    "                return Decorator(cpy,grads,function)\n",
    "            \n",
    "            optimal_lambda = golden_section_search(function_1D_wrapper(fun,current_point,gradient_normed),1,eps=1e-6)\n",
    "            current_point -= optimal_lambda*gradient_normed\n",
    "        else:\n",
    "            current_point-= gradient\n",
    "            \n",
    "        if np.isnan(current_point).any() or (current_point==np.inf).any():\n",
    "            print \"Divergence detected, stopping search with best found solution %s\" %(best_sol)\n",
    "            return best_sol\n",
    "        \n",
    "        if best_sol_value <= fun.evaluate(current_point):\n",
    "            current_patience +=1\n",
    "            if(current_patience > patience):\n",
    "                print \"No improvement, stopping search with best found solution %s\" %(best_sol)\n",
    "                return best_sol\n",
    "        else:\n",
    "            current_patience = 0\n",
    "            best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    \n",
    "            \n",
    "#newton_rhapson(F2(),[3.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_explicit_valid(explicit_restrictions,current_point, per_variable_restriction):\n",
    "    if per_variable_restriction:\n",
    "        for i,point in enumerate(current_point):\n",
    "            if i < len(explicit_restrictions):\n",
    "                if not (explicit_restrictions[i][0] < point < explicit_restrictions[i][1]):\n",
    "                    return False\n",
    "    else:\n",
    "        for point in current_point:\n",
    "            if not (explicit_restrictions[0] < point < explicit_restrictions[1]):\n",
    "                return False\n",
    "    return True\n",
    "            \n",
    "def calculate_centroid(worst_point_index, points):\n",
    "    centroid = np.zeros(points[0].shape)\n",
    "    for i,point in enumerate(points):\n",
    "        if i != worst_point_index:\n",
    "            centroid += point\n",
    "    return centroid/(len(points)-1)\n",
    "\n",
    "def move_to_centroid(point,current_point):\n",
    "    new_point = 1./2 * (point + current_point)\n",
    "    return new_point\n",
    "\n",
    "def stopping_condition(centroid_score,scores, epsilon):\n",
    "    diff = np.sum(np.array(scores)-centroid_score)\n",
    "    return np.sqrt(diff/len(scores)) <= epsilon\n",
    "        \n",
    "def box_opt(fun, starting_point, explicit_restrictions, epsilon = 1e-6, alpha = 1.3, patience=50):\n",
    "    current_point = np.array(starting_point, dtype=np.float32).copy()\n",
    "    dimensions = current_point.shape[0]\n",
    "    per_variable_restriction = True\n",
    "    if(len(np.array(explicit_restrictions).shape) == 1):\n",
    "        per_variable_restriction = False\n",
    "    \n",
    "    if not (check_explicit_valid(explicit_restrictions, current_point, per_variable_restriction) \n",
    "                    or fun.check_inequality_constraints(current_point)):\n",
    "        raise Exception(\"Argument needs to be within the restricted area\")\n",
    "    \n",
    "    points = []\n",
    "    for i in range(0,2*dimensions+1):\n",
    "        if per_variable_restriction:\n",
    "            point = []\n",
    "            for dim in range(dimensions):\n",
    "                if dim < len(explicit_restrictions):\n",
    "                    point.append(np.random.uniform(explicit_restrictions[dim][0],explicit_restrictions[dim][1],1)[0])\n",
    "                else:\n",
    "                    point.append(np.random.uniform(-1000,1000))\n",
    "                    \n",
    "            point = np.array(point) \n",
    "            while fun.check_inequality_constraints(point) == False:\n",
    "                point = move_to_centroid(point,current_point)\n",
    "            points.append(point)\n",
    "            \n",
    "        else:\n",
    "            point = np.random.uniform(explicit_restrictions[0],explicit_restrictions[1],dimensions)\n",
    "            while fun.check_inequality_constraints(point) == False:\n",
    "                point = move_to_centroid(point,current_point)\n",
    "            points.append(point)\n",
    "    \n",
    "    scores = map(fun.evaluate, points)\n",
    "    current_patience = 0\n",
    "    best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    while True:\n",
    "        worst_point_index = np.argmax(scores)\n",
    "        centroid = calculate_centroid(worst_point_index, points)\n",
    "        centroid_score = fun.evaluate(centroid)\n",
    "        \n",
    "        if best_sol_value < centroid_score:\n",
    "            current_patience +=1\n",
    "            if(current_patience > patience):\n",
    "                print \"No improvement, stopping search with best found solution %s\" %(best_sol)\n",
    "                return centroid\n",
    "        else:\n",
    "            current_patience = 0\n",
    "            best_sol, best_sol_value = centroid, centroid_score\n",
    "        \n",
    "        reflection = (1+alpha) * centroid - alpha*points[worst_point_index]\n",
    "        for i in range(dimensions):\n",
    "            if per_variable_restriction and i < len(explicit_restrictions):\n",
    "                if reflection[i] < explicit_restrictions[i][0]:\n",
    "                              reflection[i] = explicit_restrictions[i][0]\n",
    "                elif reflection[i] > explicit_restrictions[i][1]:\n",
    "                              reflection[i] = explicit_restrictions[i][1]\n",
    "            else:\n",
    "                if reflection[i] < explicit_restrictions[0]:\n",
    "                              reflection[i] = explicit_restrictions[0]\n",
    "                elif reflection[i] > explicit_restrictions[1]:\n",
    "                              reflection[i] = explicit_restrictions[1]\n",
    "                        \n",
    "        while fun.check_inequality_constraints(reflection) == False:\n",
    "                reflection = move_to_centroid(reflection,centroid)\n",
    "                \n",
    "        points[worst_point_index] = reflection\n",
    "        scores[worst_point_index] = fun.evaluate(reflection)\n",
    "        if worst_point_index == np.argmax(scores):\n",
    "                reflection = move_to_centroid(reflection,centroid)\n",
    "                points[worst_point_index] = reflection\n",
    "                \n",
    "        if stopping_condition(centroid,points, epsilon):\n",
    "            return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizacija problema bez ogranicenja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def min_no_restriction_func(fun,starting_point,epsilon=1e-6):\n",
    "    \n",
    "    current_point = np.array(starting_point,dtype=np.float32)\n",
    "    fun.set_t(1.)\n",
    "    while True:\n",
    "        new_point = nelder_mead(fun,current_point)\n",
    "        if (abs(new_point-current_point)<epsilon).all():\n",
    "            print \"Difference between new and old points less than %s, breaking the search\" % (epsilon)\n",
    "            return new_point\n",
    "        current_point=new_point\n",
    "        fun.set_t(fun.t*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zad 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bez optimalnog koraka\n",
      "No improvement, stopping search with best found solution [ 0.  0.]\n",
      "[ 0.  0.]\n",
      "\n",
      "sa optimalnim korakom\n",
      "Euclidean norm 5.3312e-07 less than od 0.01 stopping search\n",
      "[ 1.99999988 -2.99999976]\n"
     ]
    }
   ],
   "source": [
    "print \"bez optimalnog koraka\"\n",
    "print grad_descent(F3(),[0.,0.], use_golden_section=False)\n",
    "print\n",
    "print \"sa optimalnim korakom\"\n",
    "print grad_descent(F3(),[0.,0.],use_golden_section=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad 2 - Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosenbrock\n",
      "Gradijentni spust\n",
      "sa optimalnim korakom\n",
      "Euclidean norm 0.0497096 less than od 0.05 stopping search\n",
      "[ 0.96152251  0.9244483 ]\n",
      "evaluations = 216094, gradients = 2004\n",
      "_____________________________\n",
      "Newton rhapson\n",
      "sa optimalnim korakom\n",
      "Euclidean norm 0.00461999 less than od 0.01 stopping search\n",
      "[ 1.00122094  1.0036006 ]\n",
      "evaluations = 1065, gradients = 14, hessean = 14\n"
     ]
    }
   ],
   "source": [
    "print \"Rosenbrock\"\n",
    "print \"Gradijentni spust\"\n",
    "print \"sa optimalnim korakom\"\n",
    "fun = Rosenbrock()\n",
    "print grad_descent(fun,[-1.9,2.],use_golden_section=True,grad_norm=5e-2)\n",
    "print \"evaluations = %s, gradients = %s\" % (fun.calls, fun.gradient_calls)\n",
    "print \"_____________________________\"\n",
    "print \"Newton rhapson\"\n",
    "print \"sa optimalnim korakom\"\n",
    "fun = Rosenbrock()\n",
    "print newton_rhapson(fun,[-1.9,2.],use_golden_section=True)\n",
    "print \"evaluations = %s, gradients = %s, hessean = %s\" % (fun.calls, fun.gradient_calls,fun.hessean_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad2 - F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcija 2\n",
      "Gradijentni spust\n",
      "sa optimalnim korakom\n",
      "Euclidean norm 0.00416049 less than od 0.01 stopping search\n",
      "[ 3.99819663  2.00025924]\n",
      "evaluations = 1391, gradients = 15\n",
      "_____________________________\n",
      "Newton rhapson\n",
      "sa optimalnim korakom\n",
      "Euclidean norm 5.3312e-07 less than od 0.01 stopping search\n",
      "[ 4.00000048  2.00000024]\n",
      "evaluations = 119, gradients = 3, hessean = 3\n"
     ]
    }
   ],
   "source": [
    "print \"Funkcija 2\"\n",
    "print \"Gradijentni spust\"\n",
    "print \"sa optimalnim korakom\"\n",
    "fun = F2()\n",
    "print grad_descent(fun,[0.1,0.3],use_golden_section=True)\n",
    "print \"evaluations = %s, gradients = %s\" % (fun.calls, fun.gradient_calls)\n",
    "print \"_____________________________\"\n",
    "print \"Newton rhapson\"\n",
    "print \"sa optimalnim korakom\"\n",
    "fun = F2()\n",
    "print newton_rhapson(fun,[0.1,0.3],use_golden_section=True)\n",
    "print \"evaluations = %s, gradients = %s, hessean = %s\" % (fun.calls, fun.gradient_calls,fun.hessean_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad 3 Box na Rosenbrock i F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosenbrock\n",
      "No improvement, stopping search with best found solution [ 1.00543433  1.01073486]\n",
      "[ 1.00587781  1.01185676]\n",
      "Evaluations =  277\n",
      "\n",
      "F2\n",
      "[ 1.99999995  2.00021966]\n",
      "Evaluations =  266\n"
     ]
    }
   ],
   "source": [
    "print \"Rosenbrock\"\n",
    "rosen = BoxOptFunction(Rosenbrock(),[First_restriction(inequality_rest=False,power=1),Second_Restriction(inequality_rest=False,power=1)])\n",
    "print box_opt(rosen,[-1.9,2],[-100,100])\n",
    "print \"Evaluations = \", rosen.calls\n",
    "print\n",
    "print \"F2\"\n",
    "f2 = BoxOptFunction(F2(),[First_restriction(inequality_rest=False,power=1),Second_Restriction(inequality_rest=False,power=1)])\n",
    "print box_opt(f2,[0.1,0.3],[-100,100])\n",
    "print \"Evaluations = \",f2.calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosenbrock\n",
      "Difference between new and old points less than 1e-06, breaking the search\n",
      "[ 0.0101129   0.01014262]\n",
      "Evaluations =  2098\n",
      "\n",
      "F2\n",
      "Difference between new and old points less than 1e-06, breaking the search\n",
      "[ 1.99998636  2.01660061]\n",
      "Evaluations =  1978\n"
     ]
    }
   ],
   "source": [
    "print \"Rosenbrock\"\n",
    "rosen = MixedTransformationFunction(Rosenbrock(),[First_restriction(inequality_rest=True,power=1),Second_Restriction(inequality_rest=True,power=1)])\n",
    "print min_no_restriction_func(rosen,[-1.9,2])\n",
    "print \"Evaluations = \", rosen.calls\n",
    "print\n",
    "print \"F2\"\n",
    "f2 = MixedTransformationFunction(F2(),[First_restriction(inequality_rest=True,power=1),Second_Restriction(inequality_rest=True,power=1)])\n",
    "print min_no_restriction_func(f2,[0.1,0.3])\n",
    "print \"Evaluations = \",f2.calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadatak 5 - bez unutarnje tocke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between new and old points less than 1e-06, breaking the search\n",
      "[ 5.  5.]\n",
      "Evaluations =  52\n"
     ]
    }
   ],
   "source": [
    "f4 = MixedTransformationFunction(F4(),[Third_Restriction(inequality_rest=True,power=1),Fourth_Restriction(inequality_rest=True,power=1)],[Fifth_Restriction(inequality_rest=False)])\n",
    "print min_no_restriction_func(f4,[5.,5.])\n",
    "print \"Evaluations = \", f4.calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadatak 5 - s unutarnjom tockom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between new and old points less than 1e-06, breaking the search\n",
      "[ 2.00043168  0.99950902]\n",
      "Evaluations =  1943\n"
     ]
    }
   ],
   "source": [
    "finder = Inner_Point_Search([Third_Restriction(inequality_rest=False,power=1),Fourth_Restriction(inequality_rest=False,power=1)])\n",
    "starting = finder.get_inner_point(2)\n",
    "f4 = MixedTransformationFunction(F4(),[Third_Restriction(inequality_rest=True,power=1),Fourth_Restriction(inequality_rest=True,power=1)],[Fifth_Restriction(inequality_rest=False)])\n",
    "print min_no_restriction_func(f4,starting)\n",
    "print \"Evaluations = \", f4.calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
