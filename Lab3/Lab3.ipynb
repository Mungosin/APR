{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import types\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import Surface\n",
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='Mungos', api_key='mcnflyescl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nelder Mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "def nelder_mead(f, x_start,\n",
    "                step=1, no_improve_thr=10e-6,\n",
    "                no_improv_break=50, max_iter=0,\n",
    "                alpha=1., gamma=2., rho=-0.5, sigma=0.5,verbose=False):\n",
    "    \n",
    "    dim = len(x_start)\n",
    "    prev_best = f.evaluate(x_start)\n",
    "    no_improv = 0\n",
    "    x_start = np.array(x_start).astype(np.float64)\n",
    "    res = [[x_start, prev_best]]\n",
    "\n",
    "    for i in range(dim):\n",
    "        x = deepcopy(x_start)\n",
    "        x[i] = x[i] + step\n",
    "        score = f.evaluate(x)\n",
    "        res.append([x, score])\n",
    "    \n",
    "    \n",
    "    # simplex iter\n",
    "    iters = 0\n",
    "    while 1:\n",
    "        # order\n",
    "        res.sort(key=lambda x: x[1])\n",
    "        best = res[0][1]\n",
    "\n",
    "        # break after max_iter\n",
    "        if max_iter and iters >= max_iter:\n",
    "            return res[0][0]\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "        if best < prev_best - no_improve_thr:\n",
    "            no_improv = 0\n",
    "            prev_best = best\n",
    "        else:\n",
    "            no_improv += 1\n",
    "\n",
    "        if no_improv >= no_improv_break:\n",
    "            return res[0][0]\n",
    "\n",
    "        # centroid\n",
    "        x0 = [0.] * dim\n",
    "        for tup in res[:-1]:\n",
    "            for i, c in enumerate(tup[0]):\n",
    "                x0[i] += c / (len(res)-1)\n",
    "                \n",
    "        # break after no_improv_break iterations with no improvement\n",
    "        if verbose:\n",
    "            print  'Value in centroid: ', f.evaluate(x0), ' Centroid: ', x0\n",
    "                \n",
    "        # reflection\n",
    "        xr = x0 + alpha*(x0 - res[-1][0])\n",
    "        rscore = f.evaluate(xr)\n",
    "        if rscore < res[0][1]:\n",
    "            xe = x0 + gamma*(x0 - res[-1][0])\n",
    "            escore = f.evaluate(xe)\n",
    "            if escore < res[0][1]:\n",
    "                del res[-1]\n",
    "                res.append([xe, escore])\n",
    "                continue\n",
    "            else:\n",
    "                del res[-1]\n",
    "                res.append([xr, rscore])\n",
    "                continue\n",
    "        else:\n",
    "            if rscore > res[-2][1]:\n",
    "                if rscore < res[-1][1]:\n",
    "                    del res[-1]\n",
    "                    res.append([xr, rscore])\n",
    "                # contraction\n",
    "                xc = x0 + rho*(x0 - res[-1][0])\n",
    "                cscore = f.evaluate(xc)\n",
    "                if cscore < res[-1][1]:\n",
    "                    del res[-1]\n",
    "                    res.append([xc, cscore])\n",
    "                    continue\n",
    "                else:\n",
    "                    # reduction\n",
    "                    x1 = res[0][0]\n",
    "                    nres = []\n",
    "                    for tup in res:\n",
    "                        redx = x1 + sigma*(tup[0] - x1)\n",
    "                        score = f.evaluate(redx)\n",
    "                        nres.append([redx, score])\n",
    "                    res = nres\n",
    "            else:\n",
    "                del res[-1]\n",
    "                res.append([xr, rscore])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#     fun = Function(f4)\n",
    "    print nelder_mead(Rosenbrock(), [5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def golden_section_search(f,starting_point = None, a=None,b=None,eps=1, verbose = False):\n",
    "    if starting_point != None:\n",
    "        a,b = find_unimodal(starting_point,f.evaluate,eps)\n",
    "    elif a==None or b== None:\n",
    "        raise Exception(\"Starting point or unimodal interval needs to be given\")\n",
    "    \n",
    "    if verbose:\n",
    "        print \"Searching in interval [%f, %f]\" % (a,b)\n",
    "    \n",
    "    \n",
    "    fi = (math.sqrt(5.0) - 1.0)/2\n",
    "    c = b - (b - a)*fi\n",
    "    d = a + (b - a)*fi\n",
    "    while (b - a) > eps:\n",
    "        c_score, d_score = f.evaluate(c),f.evaluate(d)\n",
    "        if verbose:\n",
    "            print \"|a = %.3f|c = %.3f|d = %.3f|b = %.3f|f(c) = %.3f|f(d) = %.3f|f(c) > f(d) = %s\" %(a,c,d,b,c_score,d_score, c_score>d_score)\n",
    "        if c_score >= d_score:\n",
    "            a = c\n",
    "            c = d\n",
    "            d = a + (b - a)*fi\n",
    "        else:\n",
    "            b = d\n",
    "            d = c\n",
    "            c = b - (b - a)*fi\n",
    "    if verbose:\n",
    "        print ''\n",
    "        print \"Final interval = [\",a,\", \", b,\"]\"\n",
    "    return (a+b)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unimodalni interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_unimodal(starting_point, function, starting_step=1):\n",
    "    step = starting_step\n",
    "    current_point = starting_point\n",
    "    direction = 1\n",
    "    if function(current_point + step) >= function(current_point):\n",
    "        direction=-1\n",
    "    next_point = current_point + direction*step\n",
    "    cnt = 1\n",
    "    previous_point = current_point\n",
    "    while function(current_point) > function(next_point):\n",
    "        previous_point = current_point\n",
    "        current_point = next_point\n",
    "        next_point = starting_point + direction * (2**cnt) * step\n",
    "        cnt+=1\n",
    "    \n",
    "    if previous_point > next_point:\n",
    "        return next_point, previous_point\n",
    "    return previous_point, next_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_numeric(x):\n",
    "    NumberTypes = (types.IntType, types.LongType, types.FloatType)\n",
    "    return isinstance(x, NumberTypes)\n",
    "\n",
    "class FunctionWithRestrictions():\n",
    "    def __init__(self, func,inequality_restriction, equality_restriction):\n",
    "        self.calls = 0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        self.inequality_restriction = inequality_restriction\n",
    "        self.equality_restriction = equality_restriction\n",
    "        self.function = func\n",
    "        \n",
    "    def check_inequality_constraints(self,x):\n",
    "        for constraint in self.inequality_limitations:\n",
    "            if(constraint.evaluate(x)<0):\n",
    "                return false\n",
    "            \n",
    "\n",
    "    def evaluate(self,X):\n",
    "        self.calls+=1\n",
    "        return self.function(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rosenbrock():\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        x1= symbols('x1')\n",
    "        x2 = symbols('x2')\n",
    "        self.variables = [x1,x2]\n",
    "        self.equation = (100*(x2-x1**2)**2+(1-x1)**2)\n",
    "        \n",
    "    def subs_dict(self,x):\n",
    "        dic = {}\n",
    "        for i,var in enumerate(variables):\n",
    "            dic[var] = x[i]\n",
    "        return dic\n",
    "    \n",
    "    def gradient_symbolic(self):\n",
    "        grads = []\n",
    "        for var in variables:\n",
    "            dif = diff(self.equation,var)\n",
    "            grads.append(dif)\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.gradient_symbolic()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "        \n",
    "    def hessean_symbolic(self):\n",
    "        grads = self.gradient_symbolic()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "        \n",
    "    def hessean(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        hess = self.hessean_symbolic()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def evaluate(self,x):\n",
    "        self.calls+=1\n",
    "        return np.array(self.equation.evalf(subs=self.subs_dict(x)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F2():\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        x1= symbols('x1')\n",
    "        x2 = symbols('x2')\n",
    "        self.variables = [x1,x2]\n",
    "        self.equation = ((x1-4.)**2 + 4.*(x2-2)**2)\n",
    "        \n",
    "    def subs_dict(self,x):\n",
    "        dic = {}\n",
    "        for i,var in enumerate(variables):\n",
    "            dic[var] = x[i]\n",
    "        return dic\n",
    "    \n",
    "    def gradient_symbolic(self):\n",
    "        grads = []\n",
    "        for var in variables:\n",
    "            dif = diff(self.equation,var)\n",
    "            grads.append(dif)\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.gradient_symbolic()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "        \n",
    "    def hessean_symbolic(self):\n",
    "        grads = self.gradient_symbolic()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "        \n",
    "    def hessean(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        hess = self.hessean_symbolic()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def evaluate(self,x):\n",
    "        self.calls+=1\n",
    "        return np.array(self.equation.evalf(subs=self.subs_dict(x)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F3():\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        x1= symbols('x1')\n",
    "        x2 = symbols('x2')\n",
    "        self.variables = [x1,x2]\n",
    "        self.equation = ((x1-2.)**2 + (x2+3)**2)\n",
    "        \n",
    "    def subs_dict(self,x):\n",
    "        dic = {}\n",
    "        for i,var in enumerate(variables):\n",
    "            dic[var] = x[i]\n",
    "        return dic\n",
    "    \n",
    "    def gradient_symbolic(self):\n",
    "        grads = []\n",
    "        for var in variables:\n",
    "            dif = diff(self.equation,var)\n",
    "            grads.append(dif)\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.gradient_symbolic()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "        \n",
    "    def hessean_symbolic(self):\n",
    "        grads = self.gradient_symbolic()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "        \n",
    "    def hessean(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        hess = self.hessean_symbolic()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def evaluate(self,x):\n",
    "        self.calls+=1\n",
    "        return np.array(self.equation.evalf(subs=self.subs_dict(x)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class F4():\n",
    "    def __init__(self):\n",
    "        self.calls=0\n",
    "        self.gradient_calls=0\n",
    "        self.hessean_calls=0\n",
    "        x1= symbols('x1')\n",
    "        x2 = symbols('x2')\n",
    "        self.variables = [x1,x2]\n",
    "        self.equation = ((x1-3.)**2 + (x2)**2)\n",
    "        \n",
    "    def subs_dict(self,x):\n",
    "        dic = {}\n",
    "        for i,var in enumerate(variables):\n",
    "            dic[var] = x[i]\n",
    "        return dic\n",
    "    \n",
    "    def gradient_symbolic(self):\n",
    "        grads = []\n",
    "        for var in variables:\n",
    "            dif = diff(self.equation,var)\n",
    "            grads.append(dif)\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        grads = self.gradient_symbolic()\n",
    "        for i,grad in enumerate(grads):\n",
    "            grads[i]=grad.evalf(subs=sub_dict)\n",
    "        return np.array(grads,dtype=np.float32)\n",
    "        \n",
    "    def hessean_symbolic(self):\n",
    "        grads = self.gradient_symbolic()\n",
    "        hessean = []\n",
    "        for i,grad in enumerate(grads):\n",
    "            hess_row = []\n",
    "            for var in variables:\n",
    "                dif = diff(grad,var)\n",
    "                hess_row.append(dif)\n",
    "            hessean.append(hess_row)\n",
    "        return hessean\n",
    "        \n",
    "    def hessean(self,x):\n",
    "        self.gradient_calls+=1\n",
    "        hess = self.hessean_symbolic()\n",
    "        sub_dict = self.subs_dict(x)\n",
    "        for i,row in enumerate(hess):\n",
    "            for j, val in enumerate(row):\n",
    "                hess[i][j] = hess[i][j].evalf(subs=sub_dict)\n",
    "        return np.array(hess,dtype=np.float32)\n",
    "        \n",
    "    def evaluate(self,x):\n",
    "        self.calls+=1\n",
    "        return np.array(self.equation.evalf(subs=self.subs_dict(x)),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trazenje unutarnje tocke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.6875   8.8125]\n"
     ]
    }
   ],
   "source": [
    "class trazenje_unutarnje_tocke():\n",
    "    def __init__(self, nejednadzbe,t=None):\n",
    "        self.nejed = nejednadzbe\n",
    "        if t:\n",
    "            self.t = t\n",
    "        else:\n",
    "            self.t = 5\n",
    "            \n",
    "        for i,nejed in enumerate(self.nejed):\n",
    "            if not isinstance(nejed, Function):\n",
    "                self.nejed[i] = Function(nejed)\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        total_val = 0\n",
    "        for nejed in self.nejed:\n",
    "            value = nejed.evaluate(x)\n",
    "            if value < 0:\n",
    "                total_val -= value * self.t\n",
    "        return total_val\n",
    "    \n",
    "    def get_inner_point(self,dimensions):\n",
    "        return nelder_mead(self,np.random.randint(-10,10,size=dimensions))\n",
    "    \n",
    "def temp(x):\n",
    "    return x[1]-3\n",
    "\n",
    "def temp2(x):\n",
    "    return x[0]-3\n",
    "\n",
    "nejed = [Function(temp), Function(temp2)]\n",
    "\n",
    "inner = trazenje_unutarnje_tocke(nejed,5)\n",
    "print inner.get_inner_point(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradijentni spust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean norm 3.3326e-07 less than od 1e-06 stopping search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.00000009,  2.00000004])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_descent(fun, starting_point, epsilon = 1e-6, use_golden_section=True, patience = 50, verbose = False):\n",
    "    norma = np.linalg.norm(fun.gradient(starting_point))\n",
    "    current_point = np.array(starting_point).copy()\n",
    "    if norma < epsilon:\n",
    "        print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "        return current_point\n",
    "    \n",
    "    best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    current_patience = 0\n",
    "    while True:\n",
    "        gradient = fun.gradient(current_point)\n",
    "        norma = np.linalg.norm(gradient)\n",
    "        if verbose:\n",
    "            print \"Current best point %s with value %s\" % (best_sol,best_sol_value)\n",
    "        if norma < epsilon:\n",
    "            print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "            return current_point\n",
    "        \n",
    "        \n",
    "        if use_golden_section:\n",
    "            gradient_normed = gradient/np.linalg.norm(gradient)\n",
    "            \n",
    "            def function_1D_wrapper(function, array,grads):\n",
    "                cpy = array.copy()\n",
    "                class Decorator:\n",
    "                    def __init__(self,cpy,grads,function):\n",
    "                        self.grads = grads\n",
    "                        self.cpy = cpy\n",
    "                        self.function = function\n",
    "\n",
    "                    def evaluate(self,x):\n",
    "                        val = self.cpy - x * self.grads\n",
    "                        return self.function.evaluate(val)\n",
    "                return Decorator(cpy,grads,function)\n",
    "            \n",
    "            optimal_lambda = golden_section_search(function_1D_wrapper(fun,current_point,gradient_normed),1,eps=1e-6)\n",
    "            current_point -= optimal_lambda*gradient_normed\n",
    "        else:\n",
    "            current_point-= gradient\n",
    "            \n",
    "        if np.isnan(current_point).any() or (current_point==np.inf).any():\n",
    "            print \"Divergence detected, stopping search with best found solution %s\" %(best_sol)\n",
    "            return best_sol\n",
    "        \n",
    "        if best_sol_value < fun.evaluate(current_point):\n",
    "            current_patience +=1\n",
    "            if(current_patience > patience):\n",
    "                print \"No improvement, stopping search with best found solution %s\" %(best_sol)\n",
    "                return best_sol\n",
    "        else:\n",
    "            current_patience = 0\n",
    "            best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    \n",
    "            \n",
    "grad_descent(F2(),[3.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Rhapson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean norm 8.033e-07 less than od 1e-06 stopping search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.99999989,  2.00000001])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newton_rhapson(fun, starting_point, epsilon = 1e-6, use_golden_section=True, patience = 50, verbose = False):\n",
    "    norma = np.linalg.norm(fun.gradient(starting_point))\n",
    "    current_point = np.array(starting_point).copy()\n",
    "    if norma < epsilon:\n",
    "        print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "        return current_point\n",
    "    \n",
    "    best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    current_patience = 0\n",
    "    while True:\n",
    "        gradient = fun.gradient(current_point).dot(fun.hessean(current_point))\n",
    "        norma = np.linalg.norm(gradient)\n",
    "        if verbose:\n",
    "            print \"Current best point %s with value %s\" % (best_sol,best_sol_value)\n",
    "        if norma < epsilon:\n",
    "            print \"Euclidean norm %s less than od %s stopping search\" % (norma, epsilon)\n",
    "            return current_point\n",
    "        \n",
    "        \n",
    "        if use_golden_section:\n",
    "            gradient_normed = gradient/np.linalg.norm(gradient)\n",
    "            \n",
    "            def function_1D_wrapper(function, array,grads):\n",
    "                cpy = array.copy()\n",
    "                class Decorator:\n",
    "                    def __init__(self,cpy,grads,function):\n",
    "                        self.grads = grads\n",
    "                        self.cpy = cpy\n",
    "                        self.function = function\n",
    "\n",
    "                    def evaluate(self,x):\n",
    "                        val = self.cpy - x * self.grads\n",
    "                        return self.function.evaluate(val)\n",
    "                return Decorator(cpy,grads,function)\n",
    "            \n",
    "            optimal_lambda = golden_section_search(function_1D_wrapper(fun,current_point,gradient_normed),1,eps=1e-6)\n",
    "            current_point -= optimal_lambda*gradient_normed\n",
    "        else:\n",
    "            current_point-= gradient\n",
    "            \n",
    "        if np.isnan(current_point).any() or (current_point==np.inf).any():\n",
    "            print \"Divergence detected, stopping search with best found solution %s\" %(best_sol)\n",
    "            return best_sol\n",
    "        \n",
    "        if best_sol_value < fun.evaluate(current_point):\n",
    "            current_patience +=1\n",
    "            if(current_patience > patience):\n",
    "                print \"No improvement, stopping search with best found solution %s\" %(best_sol)\n",
    "                return best_sol\n",
    "        else:\n",
    "            current_patience = 0\n",
    "            best_sol, best_sol_value = current_point.copy(), fun.evaluate(current_point)\n",
    "    \n",
    "            \n",
    "newton_rhapson(F2(),[3.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_explicit(explicit_restriction,current_point, per_variable_restriction):\n",
    "    if per_variable_restriction:\n",
    "        for i,point in enumerate(current_point):\n",
    "            if not (explicit_restriction[i][0] < point < explicit_restriction[i][1]):\n",
    "                return False\n",
    "    else:\n",
    "        for point in current_point:\n",
    "            if not (explicit_restriction[0] < point < explicit_restriction[1]):\n",
    "                return False\n",
    "        \n",
    "def box_opt(fun, starting_point, explicit_restrictions, epsilon = 1e-6):\n",
    "    current_point = np.array(starting_point).copy()\n",
    "    dimensions = current_point.shape[0]\n",
    "    per_variable_restriction = True\n",
    "    if(len(explicit_restrictions) != dimensions):\n",
    "        per_variable_restriction = False\n",
    "    \n",
    "    if (check_explicit(explicit_restrictions, current_point, per_variable_restriction) \n",
    "                    or fun.check_inequality_constraints(current_point)):\n",
    "        raise Exception(\"Argument needs to be within the restricted area\")\n",
    "    \n",
    "    points = []\n",
    "    for i in range(0,2*dimensions+1):\n",
    "        if per_variable_restriction:\n",
    "            point = []\n",
    "            for dim in range(dimensions):\n",
    "                point.append(np.random.randint(explicit_restriction[dim][0],explicit_restriction[dim][1],1))\n",
    "            points.append(np.array(point))\n",
    "        else:\n",
    "            points.append(np.random.randint(explicit_restriction[0],explicit_restriction[1],dimensions))\n",
    "    print points\n",
    "    \n",
    "box_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
